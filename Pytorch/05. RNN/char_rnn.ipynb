{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T01:51:35.460916Z",
     "start_time": "2025-04-03T01:51:34.780325Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:52:07.912817Z",
     "start_time": "2025-04-03T01:52:07.909236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str + label_str))) # 중복을 제거한 문자들 집합\n",
    "vocab_size = len(char_vocab)\n",
    "print('문자 집합의 크기 : {}'.format(vocab_size))"
   ],
   "id": "232d3873a635cd3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:53:28.898896Z",
     "start_time": "2025-04-03T01:53:28.893037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = vocab_size # input size 는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ],
   "id": "e1177de01a21b8e9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:53:51.224817Z",
     "start_time": "2025-04-03T01:53:51.222663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
    "print(char_to_index)"
   ],
   "id": "cc42310c3b16af83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:54:12.194986Z",
     "start_time": "2025-04-03T01:54:12.191554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ],
   "id": "dd5cbded88a8ce43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:55:18.789070Z",
     "start_time": "2025-04-03T01:55:18.777796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_data = [[char_to_index[c] for c in input_str]]\n",
    "y_data = [[char_to_index[c] for c in label_str]]\n",
    "print(x_data)\n",
    "print(y_data)"
   ],
   "id": "a1e2d60aeaa1319",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:55:46.751570Z",
     "start_time": "2025-04-03T01:55:46.746481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ],
   "id": "ec30ff5ba36b5524",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:56:20.425566Z",
     "start_time": "2025-04-03T01:56:20.419843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
    "print('레이블의 크기: {}'.format(Y.shape))"
   ],
   "id": "f863ee3f1f84e569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기: torch.Size([1, 5, 5])\n",
      "레이블의 크기: torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/xgf_87rd5nz9rsbc143wp9qc0000gn/T/ipykernel_46464/803733204.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729647058851/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:57:54.485862Z",
     "start_time": "2025-04-03T01:57:54.475174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # output layer\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3d Tensor"
   ],
   "id": "dbfa963042b1d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:58:17.386266Z",
     "start_time": "2025-04-03T01:58:17.382993Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.view(-1, input_size).shape) # Transform 3D to 2D tensor",
   "id": "ebeb427a85e7ed27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:58:30.543591Z",
     "start_time": "2025-04-03T01:58:30.540185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ],
   "id": "88a80eb048211d41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:01:52.659613Z",
     "start_time": "2025-04-03T02:01:52.025640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis = 2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, 'loss: ', loss.item(), 'prediction: ', result, 'true Y: ', y_data, 'prediction str: ', result_str)\n"
   ],
   "id": "e61e3899be9e8009",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.6804767847061157 prediction:  [[0 0 0 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  !!!!!\n",
      "1 loss:  1.397083044052124 prediction:  [[4 0 0 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  p!!!!\n",
      "2 loss:  1.1792432069778442 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "3 loss:  0.9863972663879395 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "4 loss:  0.8048609495162964 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "5 loss:  0.6379421353340149 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.4941175580024719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.3713584542274475 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.26811450719833374 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.19467902183532715 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.14100052416324615 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.1026877760887146 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.07642732560634613 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.057637833058834076 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.04366299510002136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.03324003890156746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.02556592784821987 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.019981136545538902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.015932749956846237 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.012982857413589954 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.010804115794599056 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.00916280783712864 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.007897326722741127 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.006898246705532074 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0060923146083951 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.0054299854673445225 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.004877569619566202 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.004411348607391119 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.004014075733721256 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.00367286941036582 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.003377786371856928 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0031211397144943476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0028966059908270836 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0026992280036211014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.002524967771023512 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.002370374044403434 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0022328479681164026 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0021100237499922514 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.002000032924115658 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0019012654665857553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0018123483750969172 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.001732263364829123 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0016598965739831328 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0015944897895678878 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0015352602349594235 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0014815436443313956 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.001432770979590714 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0013884434010833502 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0013480862835422158 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0013112955493852496 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.001277620205655694 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0012468224158510566 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.001218545832671225 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0011924335267394781 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0011684861965477467 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.001146251568570733 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.001125610782764852 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0011064214631915092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0010884932707995176 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0010718024568632245 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0010561348171904683 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0010414667194709182 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0010275840759277344 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0010145343840122223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.001002103672362864 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0009904346661642194 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0009792892960831523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0009686917182989419 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0009584749350324273 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0009487819625064731 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0009393985383212566 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0009303960832767189 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0009217271581292152 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.000913343857973814 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0009051748784258962 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0008972678333520889 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0008895989740267396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0008821920491755009 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0008748803520575166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0008678305894136429 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0008609473588876426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0008542072027921677 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0008476335788145661 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0008411552989855409 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0008348434930667281 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.000828722317237407 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0008226248319260776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0008166941115632653 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0008108822512440383 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0008051657350733876 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0007994967745617032 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0007939706556499004 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0007884921506047249 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0007831089315004647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0007777494029141963 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0007726042531430721 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.000767459103371948 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0007623137789778411 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0007572876638732851 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0007523806998506188 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:02:34.518121Z",
     "start_time": "2025-04-03T02:02:34.515136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n"
   ],
   "id": "d570377dbb502272",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:03:28.130317Z",
     "start_time": "2025-04-03T02:03:28.126072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "dic_size = len(char_set)\n",
    "\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10\n",
    "learning_rate = 0.1"
   ],
   "id": "473b5bf7b2d572ee",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:05:07.686459Z",
     "start_time": "2025-04-03T02:05:07.679213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i: i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])"
   ],
   "id": "d570a724020516af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:05:39.618451Z",
     "start_time": "2025-04-03T02:05:39.615456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x_data[0])\n",
    "print(y_data[0])"
   ],
   "id": "d62c3cbc54ec42af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 18, 0, 23, 17, 24, 0, 20, 2, 5]\n",
      "[18, 0, 23, 17, 24, 0, 20, 2, 5, 19]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:06:32.021651Z",
     "start_time": "2025-04-03T02:06:32.009204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
    "print('레이블의 크기: {}'.format(Y.shape))\n",
    "\n",
    "print(X[0])"
   ],
   "id": "ffabffacff2fdccd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기: torch.Size([170, 10, 25])\n",
      "레이블의 크기: torch.Size([170, 10])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:08:31.572707Z",
     "start_time": "2025-04-03T02:08:31.558052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers = layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(dic_size, hidden_size, 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ],
   "id": "6ed3231d4f99d4a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:08:45.404890Z",
     "start_time": "2025-04-03T02:08:45.401672Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.view(-1, dic_size).shape)",
   "id": "151e3111a690d072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:08:55.841635Z",
     "start_time": "2025-04-03T02:08:55.839267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ],
   "id": "3b35127ad7ed645",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:11:02.306397Z",
     "start_time": "2025-04-03T02:11:02.100142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 Tensor Size : (170, 10)\n",
    "    results = outputs.argmax(dim = 2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ],
   "id": "a07dd8b3dd88b333",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml ll  llll ll l mll l lmllm lllll lm lmcl llmllm llmll lm ll lllllll mmlm cllmlllll cllmmm l ll lllhl cllmlll mml,l l c,ll llm c l,ll ll lllm llm l,l lllllll lllmmlmll ll l l llc\n",
      "                                                                                                                                                                                   \n",
      "hhthihthihhbbrbggbbhibbbibshibrbgbfbshgbgibgghbggbhgbrbibbthgbtibgbhhgbrbggbhhbb'bhbgbghgbgbbibbshgbiggbghbbrbgbbggbbbrbgbgihgbthbb,hg'hgbhgbgbggbgbibghhbbsggbbshbbsgb,hgsfb,egggb\n",
      "ooeoeoeoeoeooeoeoeeoeoeeeoeooeeoeoeoeoeoeeoeoeooeoeeeeoeoeooeoeoeeoeoeeeoeooeoeoeoeoeoeooeoeoeoeoeeoeoeoeeooeoeoeoeoeoeoeoeooeoeoeoeoeoeeoeooeoeooeoeoeooeoeoeeeeeoeeeoeeeoeoeoeoee\n",
      " ltltltltltltotoltototooto otoootototltltototoototooooo otltoooltototooooolootoltototototoootoooooololtototototototototototltololtototoool ltototooo ototototoloootooooolotootototo\n",
      "  t t t t i           t i     w w w w                    w           t t t    t t w w w i             t w   w w t w w w        t w i i      w     t t i i                  t w   w \n",
      "s s      s sh   sh     s s s s    i  s  h    ss shs, ,  shs    r s        shs          s        sh     s              ir s s        s shs    s  s    shs      f    s   s      s s  \n",
      "ddth,h.h....h,.....h.h.....h...h.h.h,. t.sh.......h.h.h,h.th,h.h....h,..h.t.th.h.h......g .th.....d.nhg..h........,.,h.h..h.h.h.h,..h .h,h,h.....h..n...tn. ..................hwh.h\n",
      "p a   d  a d    d d a a  a da   d d dada   da d         a a    a   d a  a a   a   d d d a da    a    aa d       d        d a     a  a a    a  a      ad a    a    da a da  a  ad   \n",
      "p t e e e  t e    e t e e e e e e e et t   et      e e  t t e ea   e t e  t   t e e e e t et e  t e e e e e e e e   e    t t e e e e  t e ea et e e ea    e et  e e  t et  t et  e \n",
      "p t e t e  t e  t t t e e e t e e t et t   e e     e e e  t e    t   t e  t   t e t t e t tt e  t e e t e t e e t t e e e  t e e t e  t e e  tt e e e   e e et  e t  t tt  t e   e \n",
      "p t e t t  t e tt ttt t e e t e t t ttttt  t       e t e  t e t tt t t e tt t t e t t e t tt e  t e   t t t e e ttt t e e  t t e t e  t e   ttt e t e t t t ttt t t tt tt tt e t t \n",
      "p to ototh to ottot t t     t  ot t tt t  os         t    tot s tt s totottotototot t     tt    t t s t t toto otttot      tot   t    t tos  ttonot         t   t    t stotto    t \n",
      "p to ototo totottotototo o  tonot tott toto      t  oto t totottot   toto totototot t     tto t toto  totototototttototo   toto  to t totosnottonoto t    to t oto   to totto o    \n",
      "p to oto o to o to  t       t  ot   t  t ro          t    toro no     oto torotorot        t    to    t   to oro t oto     tor   t    toro ro tonoto                    to to      \n",
      "p torotoao torottoa tod ap  tonot t tt toto      toroto r toro aoa   tott torototot t      to r toro  toaototkaotatototo r toro  to r toro aoatorpto a  o t     to  o o to to    r \n",
      "pdtodptoda todottod t d tmd thdothd tt aoto      t d t dr todo aod   todm tod todot d      todr todo  tod tod'to  todot dr tod   todm todp aodtod'todr  d t   t d  t    t  todr  d \n",
      "pd odptodm t dlttm  t e t e d dlt d dt todp      d d t    todl n     todm t   t d't d      d d  tod   t   tod't   t d t    t d   tod  todb a  t d't dm  d t   t d       d  t dm  d \n",
      "pe  n'toem t dbttoe t e t e thn't d tt tldl      d d t    t d'tn     todm t d t n't e      the  tod   t e tod't  tt d't e  t d   t em todb noet d't em  d t   t d       t  t em  t \n",
      "pe  n'toet toe'ttoe t t t e thn'thd tt totl  o   t n t e  thn'tn   o toel t d thn'the    e the  toe   tn  toe't  ut t'the  thn   toem ton'tn'et n'themt tlt   t t  l t  to them  t \n",
      "pe  r'toet toe'ttoe t t e e thn'the tt to '  e   t n the  thn'tn   t toe' th  ton'the    e the  toe   tn  toe't  utotkthe  the o toe  ton' n  ton'the t tld   t t nl t  t  the t   \n",
      "g   r'toet toepttoe t d   e t n't e tm torp  e   t n t e  tonptn e t toet th  ton'thd    e the  toe   tn  toekr  utotkthe  to  o toe  tonp n  ton'the o t e   toe rl    t  thero   \n",
      "g   r'toet torpttpe t d rme tonpt t rm torp  o   t r ther torptroe totorm tht tor't t    e ther toep  tn  torar  utotkther tormo toer torp n  torkthero toe   toe rl t  tm thero r \n",
      "g   rptoet torputpdht e rme tonpt a rm torp  o   thr them thrptmoe thtoem thd tor't an   n ther toe e tn  toear  utot them thr o toer torpon  torkthemo doe   toe rl t  tm themo r \n",
      "g e t'toet torcutpa tne rpe tonpt a tm totp  o   thr them thrbtmoe t toem thd torpt an  mn them toe   tn  toekr dutoakthem toe o toem tonpon  con themondoe  ttoe rl to tm themo rn\n",
      "g e t'toet t tcutpa tnd tpe donkt a tm totp uo   tor them thrbtm e t toet t d thnkt a   mn them toe   tn  toe   dut tkthem toe o toem tonbon  con themonuoe  ttoe np to tm themo te\n",
      "g e t'toet to lutla tnd tpe don't e um totp uo e to  them th lom e t toel thd th 't en  gn them toe   tnd toek  dut tkthem toe o toem to bon  ion themondoe e doe n' t  tm themo te\n",
      "g e e'toem to butpa tnd  pe don't ehum t'tp up e to ethem th bol e t tonl thd thn't en  gn them to    tnd tonk  duth kther to lo toem to bon  ion themondoe e doe n' t  dm themo tl\n",
      "g e i'toeteto butpa t d  pe don't a um t'tpeup e to  ther to bof n totonl thd thn't dn  gn ther to ke tnd tonk  dut  kther to co toem to bonb ton themdhdoe e i e n'ot  dm therd tc\n",
      "g e i'toet to buiga thd tpe don't ahut t'ip ep e th ether th bof e t tonl thd thn't ans gn ther to ke dnd tonk  dut  kther th  e toer to bonb ton therehdoe s   e n' t  df there t \n",
      "gpo t'toetetorbutpd tod tpe don't ahut toip lfoeeto etoer thtfofle t tont thd ton't ans gn toer to ke and tonk  dut  kther to co toem to bont ion therdhdoe   tpten'ot  df therd tl\n",
      "gpe e'toem th builn ths gpe dhn't a um e' p      th  i er th cof e t tonl ahd thn't ans  n ther theke dnd ton e dut   ther th  e ther thnbonl c n therend e s   t n  t  df there t \n",
      "gpo totoet torlutld tha tpe don't a um to pempldoto ethem thtloflelt toat thd thn't ansign them to ke ant wonk  dut aathem th co toem to lonu lon thementle s wpter st  df theme tc\n",
      "gpo tptoet torbuild t d tpe don't a um to pelp e to ethem th loflect tonl ths ton't ansign them to ke and wonk  dut  kthem to co toem torbonu lonkthemendle   ipten st  df theme tn\n",
      "gpe  ktoet ao luild t e epe dhn't a um t' peip e th ct er th loflect tonl ths ton't ans gn ther toek  dn  tonke dut  kther th ce ther to bong lon therend e s ip en et  af there t \n",
      "gpo r'toet to luitd t ahgp  don't arum to peop e th cthem th loflest tont thd ton't ansign ther to ke and tonke dut  kther th cr ther to long ton therend ess ip  n et  af there tn\n",
      "gpo aktoet to lutld a ahup  don't aoum to people to etoem to lofle t tont ahd ton't ansign toer to ke dnd tonk  duto atoer to co toem torlong tonktherendoess tm  n styodf toere t \n",
      "gpo  ktoet to lutld a dhep  dhn't ahum tp peop e th ethem th lofle t toel ahd ton't ans gn ther to ke dnd tonke dut rather th ch toer to long lon therend ess im en ste df there tn\n",
      "gpe  ktoet to build t d gp  dhn't d um i' people th ethem th loflect tond  hd thn't dnsign them tonk  dnd ton e dut  athem th ce them to bong lon themend ess  m en st  df theme tn\n",
      "gpoo ktoet to butld t d up  dhn't d um u' peoplesth ct em th lollect tord a d ton't dnsign them to k  dnd tonke but  athem th ce them to bong lon themendless imten sty of theme tn\n",
      "gpoaaktoet to lutld a dhep  don't arum up people th ethem th lollect tord t d ton't ahsign them to ks dnd torke but  athem th ch toem to lor' lon themendless umtenssty of theme tn\n",
      "gfeaaktoet to lutld a o ip, don't arum up people th ethem to loloect tood t d ton't ansign them to ks dnd torke dut rathem to ch toem to long lon themendless umten ity af theme an\n",
      "gyeaaltoet to lutld ans ip, donmt arum up peefle th cthem t  loloect tood t d ton't ansign them tonk  and torke dut rathem t  ch them to long fon themendless upten ity ef theme an\n",
      "gmeaaatoet to build and ep, don't arum up p ofle th ethem th collect tood t d ton't ans gn them tonk  and torke dut  athem th ch them to bong fon themendless ummen sty of theme an\n",
      "gmeaaltoet to build and ip, don't arum up people to cthem t  collect tood and ton't ansign them tonk  and tork, but rathem t  ch them to long fon themendless upm n ity of toemehan\n",
      "g eaalwoet to luild a s ip, dor't arum up people th ethem t  lollect tood and ton't ahsign them tonks and tork, but rathem t uch them to long fon themendless ummen ity of there an\n",
      "g eaalwoet to build ans ip, dormt arum up people to ethem to lollect tord and dongt ansign them tonks and dork, but rather th ch them to long for therendless immen ity of there tn\n",
      "g eaaawoet th build ans ip, don't drum up people thgethem to collect tood and ton't ansign them tonks and aork, but rathem th ch them to long for therendless imm n ity of there an\n",
      "g daalwoet to build andhip, don't drum tp people to ether to bollect tood and won't dnsign them tonks and work, but rather toich them to long fon therendless immen ity of there an\n",
      "g daaawoet to build ans ip, dor't arum up peoplesth cthem th bollect tord and won't ansign them tonks and work, but rather th ch them to bong fon therendless imten ity of therehtn\n",
      "p daalwoet to build ans ip, dor't arum up peoplesth ethem to bollect tord and don't ansign them tonks and works but rather thtse them to long for therendless immen ity of there tn\n",
      "p daalwoet to luild anship, dor't arum up people to ether to lollect tord and don't dnsign them tonks and works but rather to ch them to long for therendless immen ity of toere tn\n",
      "p daalwoet to build a s ip, dor't arum up pesple to ethe  to bollect wood a d don't ansign them tonks and work, but sathe  toach them to long for the endless immen ity of the e an\n",
      "p daalwoet to build andhip, don't arum up people thtethem thtlollect tooo dnd don't ansign them tonks and wook, but rathe  thach them to long for the endless immensity of the e tn\n",
      "p daalwaet roncuild a skip, don't arum rp people th kther to collect word a d aon't ansign ther uonks and work, but rather tosch them uo bong fonktherendless immensity af toere an\n",
      "p doolwoet to lutld an hip, dor't arum up people do cthem tl lollect word t d wor't aesign them tosks asd work, but rather tl chsteem to long for therendless immensitl of there tn\n",
      "p doukwoom uo luild a  hep, dor't arum up peodlesthgether to lollect word and wor't ansign ther tosks and work, but rather to ch them to long for therdndless immensety of therd tn\n",
      "p youkwaet togbutld a ship, don't arum tp peoplecthgcthem to boblect tort tnd wor't ansign them tonks and work, but rathem to ch them to bong for themendless immensity of theme tn\n",
      "p ylokwoet to build a ship, don't drum tp people to ethe  to bollect wood and won't d sign them tonks and wonk, but rathe  toach them to bong fon the endless immensity of the e an\n",
      "p yookwoet to build anship, don't drum ap people to ethe  to bollect wood wnd won't d sign the  tonks and wook, but rathe  toach toe  to bong fon the endless immensity of the e an\n",
      "p yookwoet to luild a ship, don't arum up people th ether to lollect wood and won't ansign ther tosk, and wook, but ,ather toach them to long for therenglesg imoensity of there tn\n",
      "p youlwoet to luild a ship, don't arum up peoplesth ether to lollect wood and won't acsign ther tosks and work, but rather toach them to long for therendless immensity of there tn\n",
      "p youlwoet to luild a ship, don't arum up people th ether to bollect word and won't assign ther tosks and works but rather teach them to long for therendless immensity of there as\n",
      "p youkwaet to build a shsp, don't arum up people to ethe  to bollect worp and won't ansign the  tosks and works but rathe steach them to bong for the endless immensity of the e an\n",
      "p youkwaet to build a ship, don't arum up people th ether to bollect dord and don't ansign them tosks and work, but rathe  toach them to bong for the sndless immen ity of the s an\n",
      "g youlwant to luild an hip, don't drum up people do ether to lollect tord tnd don't ansign them tosks and work, but rather toach them to long for themendless immen ity of there tn\n",
      "m you want to luild a ship, don't drum up people th ether to lollect word and don't dnsign them tosks and work, but rather toach them to long for therendless immensity of the e an\n",
      "p you want to luild a ship, don't arum up people to ether to bollect word and won't ansign them tosks and work, but rathe  toach them to bong for the endless immensity of the e an\n",
      "p you want to build a ship, don't arum up people to ether te bollect word and don't assign them tosks and wook, but rathe  teach them to bong for the endless immensity of the s an\n",
      "p you want to luild a ship, don't arum up people thgether te bollect word and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s as\n",
      "p youawant to luild a ship, don't arum up peoplestogether to lollect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seas\n",
      "p you want to cuild a ship, don't arum tp people to ether to collect wood and don't assign them tosks and wook, but rather teach them to cong for the sndless immensity of the seas\n",
      "p youkwant to cuild a ship, don't drum up people together to collect wood and don't assign them tosks and wook, but rather teach them to cong for the endless immensity of the seas\n",
      "p youkwant to luild a ship, don't drum up people together to lollect wood and don't dssign them tosks and wook, but rather toach them to long for the endless immen ity of the eeas\n",
      "t youkwant to luild a ship, don't drum up people to ether to lollect wood and don't assign them tosks and wook, but rather toach them to long for the endless immensity of the seas\n",
      "t youkwant to build a ship, don't arum up people together to bollect wood and don't assign them tosks and wook, but rather teach them to bong for the endless immensity of the seas\n",
      "t youkwant to build a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the seas\n",
      "t you want to build a ship, don't drum up people together to bollect word and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the seas\n",
      "t you want to luild a ship, don't drum up people to ether to bollect word and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the seas\n",
      "t you want to luild a ship, don't drum up people together to bollect word and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the seas\n",
      "g you want to luild a ship, don't drum up people together to bollect word and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "g you want to luild a ship, don't drum up people together to bollect word and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l you want to luild a ship, don't drum up people together to bollect word and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seas\n",
      "f you want to cuild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to luild a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to build a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p you want to luild a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the seas\n",
      "p you want to cuild a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together te bollect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "g you want to cuild a ship, don't drum up people together to lollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e1d1af0a26040cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
