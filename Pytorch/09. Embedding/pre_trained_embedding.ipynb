{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T03:21:37.857870Z",
     "start_time": "2025-04-07T03:21:37.425619Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:22:04.076813Z",
     "start_time": "2025-04-07T03:22:04.073080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "print('단어 토큰화 된 결과 :', tokenized_sentences)"
   ],
   "id": "3262bd398d0d6b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 된 결과 : [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:22:47.311842Z",
     "start_time": "2025-04-07T03:22:47.308769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_list = []\n",
    "for sent in tokenized_sentences:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "\n",
    "word_counts = Counter(word_list)\n",
    "print('총 단어수: ', len(word_counts))"
   ],
   "id": "58352e8778034ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수:  15\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:22:18.795409Z",
     "start_time": "2025-04-07T06:22:18.790788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = sorted(word_counts, key = word_counts.get, reverse = True)\n",
    "print(vocab)"
   ],
   "id": "ac6a5a1746da9942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:24:23.474476Z",
     "start_time": "2025-04-07T06:24:23.470025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index + 2\n",
    "vocab_size = len(word_to_index)\n",
    "print('Padding Token, UNK Token을 고려한 단어 집합의 크기: ', vocab_size)"
   ],
   "id": "db6c8949cd680a23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Token, UNK Token을 고려한 단어 집합의 크기:  17\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:24:49.528626Z",
     "start_time": "2025-04-07T06:24:49.526427Z"
    }
   },
   "cell_type": "code",
   "source": "print(word_to_index)",
   "id": "5c97571977c06bbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:26:46.162273Z",
     "start_time": "2025-04-07T06:26:46.158811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "    encoded_X_data = []\n",
    "    for sent in tokenized_X_data:\n",
    "        index_sequences = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                index_sequences.append(word_to_index[''])\n",
    "        encoded_X_data.append(index_sequences)\n",
    "    return encoded_X_data\n",
    "\n",
    "X_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "print(X_encoded)"
   ],
   "id": "4e40bbcf93f4ca3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:27:28.820016Z",
     "start_time": "2025-04-07T06:27:28.816706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('Max len :', max_len)"
   ],
   "id": "d4d1ba6bc623d7ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len : 4\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:31:16.135228Z",
     "start_time": "2025-04-07T06:31:16.129698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequences(sentences, max_len):\n",
    "    features = np.zeros((len(sentences), max_len), dtype = int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features\n",
    "\n",
    "X_train = pad_sequences(X_encoded, max_len = max_len)\n",
    "y_train = np.array(y_train)\n",
    "print('Padding Output :')\n",
    "print(X_train)"
   ],
   "id": "61b8f8db183cd701",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Output :\n",
      "[[ 2  3  4  5]\n",
      " [ 6  7  0  0]\n",
      " [ 8  9  0  0]\n",
      " [10 11  0  0]\n",
      " [12 13  0  0]\n",
      " [14  0  0  0]\n",
      " [15 16  0  0]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:35:43.482774Z",
     "start_time": "2025-04-07T06:35:43.479421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "cd09870059204e38",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:35:44.307783Z",
     "start_time": "2025-04-07T06:35:44.302618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        flattened = self.flatten(embedded)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)"
   ],
   "id": "820b08a39cf1e818",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:35:44.845898Z",
     "start_time": "2025-04-07T06:35:44.837674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "embedding_dim = 100\n",
    "simple_model = SimpleModel(vocab_size, embedding_dim).to(device)"
   ],
   "id": "c9324eca525fdfa7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:35:45.970437Z",
     "start_time": "2025-04-07T06:35:45.423470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(simple_model.parameters())"
   ],
   "id": "739ce9d5a31cfcf0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:37:34.590757Z",
     "start_time": "2025-04-07T06:37:34.587127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype = torch.long), torch.tensor(y_train, dtype = torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 2)\n",
    "print(len(train_dataloader))"
   ],
   "id": "9ccbb9bbc3f77161",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:39:12.618339Z",
     "start_time": "2025-04-07T06:39:11.351605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(10):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = simple_model.forward(inputs).view(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ],
   "id": "25c07c2e1bf9db25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9972531199455261\n",
      "Epoch 2, Loss: 0.7904385328292847\n",
      "Epoch 3, Loss: 0.599193811416626\n",
      "Epoch 4, Loss: 0.4577890932559967\n",
      "Epoch 5, Loss: 0.36288413405418396\n",
      "Epoch 6, Loss: 0.30181896686553955\n",
      "Epoch 7, Loss: 0.2624887526035309\n",
      "Epoch 8, Loss: 0.23584935069084167\n",
      "Epoch 9, Loss: 0.21577180922031403\n",
      "Epoch 10, Loss: 0.19846446812152863\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:39:38.862544Z",
     "start_time": "2025-04-07T06:39:37.867350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j"
   ],
   "id": "81c7781a3917f70e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\r\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from gdown) (4.12.3)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from gdown) (3.13.1)\r\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from gdown) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from gdown) (4.67.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from requests[socks]->gdown) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from requests[socks]->gdown) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from requests[socks]->gdown) (2025.1.31)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/ml_env/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: gdown\r\n",
      "Successfully installed gdown-5.2.0\r\n",
      "zsh:1: no matches found: https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:41:03.856743Z",
     "start_time": "2025-04-07T06:41:03.830271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print('Embedding Matrix Size: ', embedding_matrix.shape)"
   ],
   "id": "9cccf96b0c4e4746",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m word2vec_model \u001B[38;5;241m=\u001B[39m \u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGoogleNews-vectors-negative300.bin.gz\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m embedding_matrix \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((vocab_size, \u001B[38;5;241m300\u001B[39m))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEmbedding Matrix Size: \u001B[39m\u001B[38;5;124m'\u001B[39m, embedding_matrix\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[1;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[1;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1676\u001B[0m     ):\n\u001B[1;32m   1677\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[1;32m   1678\u001B[0m \n\u001B[1;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1717\u001B[0m \n\u001B[1;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/gensim/models/keyedvectors.py:2048\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[1;32m   2045\u001B[0m             counts[word] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(count)\n\u001B[1;32m   2047\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading projection weights from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, fname)\n\u001B[0;32m-> 2048\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fin:\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m no_header:\n\u001B[1;32m   2050\u001B[0m         \u001B[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001B[39;00m\n\u001B[1;32m   2051\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m binary:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/smart_open/smart_open_lib.py:224\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ve:\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(ve\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m--> 224\u001B[0m binary \u001B[38;5;241m=\u001B[39m \u001B[43m_open_binary_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransport_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m filename \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    226\u001B[0m     binary\u001B[38;5;241m.\u001B[39mname\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# if name attribute is not string-like (e.g. ftp socket fileno)...\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m uri\n\u001B[1;32m    231\u001B[0m )\n\u001B[1;32m    232\u001B[0m decompressed \u001B[38;5;241m=\u001B[39m so_compression\u001B[38;5;241m.\u001B[39mcompression_wrapper(\n\u001B[1;32m    233\u001B[0m     binary,\n\u001B[1;32m    234\u001B[0m     binary_mode,\n\u001B[1;32m    235\u001B[0m     compression,\n\u001B[1;32m    236\u001B[0m     filename\u001B[38;5;241m=\u001B[39mfilename,\n\u001B[1;32m    237\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/smart_open/smart_open_lib.py:412\u001B[0m, in \u001B[0;36m_open_binary_stream\u001B[0;34m(uri, mode, transport_params)\u001B[0m\n\u001B[1;32m    410\u001B[0m scheme \u001B[38;5;241m=\u001B[39m _sniff_scheme(uri)\n\u001B[1;32m    411\u001B[0m submodule \u001B[38;5;241m=\u001B[39m transport\u001B[38;5;241m.\u001B[39mget_transport(scheme)\n\u001B[0;32m--> 412\u001B[0m fobj \u001B[38;5;241m=\u001B[39m \u001B[43msubmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen_uri\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransport_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(fobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    414\u001B[0m     fobj\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m uri\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/smart_open/local_file.py:34\u001B[0m, in \u001B[0;36mopen_uri\u001B[0;34m(uri_as_string, mode, transport_params)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mopen_uri\u001B[39m(uri_as_string, mode, transport_params):\n\u001B[1;32m     33\u001B[0m     parsed_uri \u001B[38;5;241m=\u001B[39m parse_uri(uri_as_string)\n\u001B[0;32m---> 34\u001B[0m     fobj \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparsed_uri\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muri_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69dd5a0cd75e0427"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
